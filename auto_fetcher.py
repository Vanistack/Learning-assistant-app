import os
import json
import feedparser
import openai
from dotenv import load_dotenv
from datetime import datetime

# Load API key
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# RSS feeds to pull from
RSS_FEEDS = {
    "Towards Data Science": "https://medium.com/feed/towards-data-science",
    "Data Science Central": "https://www.datasciencecentral.com/feed/",
    "HuggingFace": "https://huggingface.co/blog/rss.xml"
}

# Prompt Template
SUMMARY_PROMPT = """
You are an AI assistant helping to convert blog content into a learning module.

Step 1: Write a 500‚Äì700 word intermediate-level summary of the following blog post.

Step 2: Generate 3 multiple-choice quiz questions with 3 options each, and clearly specify the correct answer.

Step 3: Identify the topic category: choose one from ["Data Analysis", "Analytics", "Data Science", "Machine Learning", "Artificial Intelligence"]

Article:
\"\"\"
{article}
\"\"\"
"""

# Function to fetch and process news
def fetch_and_process_articles(max_articles=1):
    topics = {}

    for source, url in RSS_FEEDS.items():
        feed = feedparser.parse(url)
        articles = feed.entries[:max_articles]

        for article in articles:
            print(f"Processing: {article.title}")
            full_input = SUMMARY_PROMPT.format(article=article.summary)

            try:
                response = openai.ChatCompletion.create(
                    model="gpt-4",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant."},
                        {"role": "user", "content": full_input}
                    ],
                    temperature=0.7
                )

                result = response['choices'][0]['message']['content']
                parsed = parse_gpt_output(result)
                if parsed:
                    # Create a topic key using date + title
                    key = f"{datetime.today().strftime('%Y%m%d')}_{article.title[:30].replace(' ', '_')}"
                    topics[key] = parsed

            except Exception as e:
                print("Error processing article:", e)

    return topics

# Parse GPT output into expected format
def parse_gpt_output(text):
    try:
        # Naive parsing (could be improved with function-calling)
        parts = text.split("Step")
        summary = parts[1].strip().replace("1:", "").strip()
        quiz_part = parts[2].strip().replace("2:", "").strip()
        track = parts[3].strip().replace("3:", "").strip()

        # Extract quiz questions (basic pattern-based)
        quiz = []
        for q in quiz_part.split("\n\n"):
            lines = q.strip().split("\n")
            if len(lines) >= 4:
                question = lines[0].strip()
                options = [lines[1].strip(), lines[2].strip(), lines[3].strip()]
                answer_line = [l for l in lines if "correct answer" in l.lower()]
                answer = answer_line[0].split(":")[-1].strip() if answer_line else options[0]
                quiz.append({"question": question, "options": options, "answer": answer})

        return {
            "title": "Autogenerated - " + summary[:60],
            "track": track,
            "summary": summary,
            "quiz": quiz
        }

    except Exception as e:
        print("Error parsing GPT output:", e)
        return None

# Save to file
def save_to_json(topics, path="new_topics.json"):
    if os.path.exists(path):
        with open(path, "r") as f:
            existing = json.load(f)
    else:
        existing = {}

    existing.update(topics)

    with open(path, "w") as f:
        json.dump(existing, f, indent=2)
    print(f"Saved {len(topics)} topics to {path}")

# --- Run ---
if __name__ == "__main__":
    print("üîç Fetching articles and generating learning content...")
    new_topics = fetch_and_process_articles(max_articles=1)
    save_to_json(new_topics)
